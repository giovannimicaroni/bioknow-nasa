# ==========================================
# AI CONNECTOR CONFIGURATION
# ==========================================
# The system uses an automatic AI connector that:
# 1. Tries OpenAI first (if OPENAI_API_KEY is configured)
# 2. Falls back to LM Studio (local model) if OpenAI fails or is not configured
#
# Configure ONLY what you plan to use:
# ==========================================

# OpenAI API Configuration (PRIORITY)
# Leave empty to use LM Studio by default
OPENAI_API_KEY=

# LM Studio Configuration (FALLBACK - Local Models)
# Make sure LM Studio is running on this URL
LM_STUDIO_URL=http://localhost:3000/v1/chat/completions
LM_STUDIO_MODEL=deepseek/deepseek-r1-0528-qwen3-8b

# Ollama Configuration (OPTIONAL - Only for Langchain Agent)
# Only used if you want to use Ollama with the Langchain agent
# Leave empty to use the unified connector instead
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# ==========================================
# LEGACY PROVIDERS (Not used by default)
# ==========================================
# Anthropic API Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini API Configuration
GEMINI_API_KEY=your_gemini_api_key_here

# Flask Configuration
FLASK_SECRET_KEY=your_secret_key_here


# Environment
FLASK_ENV=production
